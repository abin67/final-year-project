import pandas as pd
pf=pd.read_csv("F:\heart_data.csv")
Data Pre-processing-----------------
pf=pf.replace({'num':{2:1,3:1,4:1}})
pf=pf.rename(columns={'num':'target'})
pf
pf=pf.drop(columns=['ccf','id','restckm','exerckm','thalsev','thalpul','earlobe','ivx1','ivx2','ivx3','ivx4','ivf','cathef','junk','painloc','painexer','relrest','pncaden','smoke','dm','rldv5','restef','restwm','exeref','exerwm','diag','ramus','om2'])
Exploraratory data analysis-------------
import seaborn as sns
sns.boxplot(x='target',y='age',data=pf)

X=pf.drop(labels='target',axis=1)
Y=pf['target']
Normalisation-----------------------------------------

from sklearn.preprocessing import StandardScaler
scaler=StandardScaler()
scaler.fit(X)
scaled_data=scaler.transform(X)
Dimensionality techniques--------------------
    Feature selection techniques             

1)Random Forest importance-------------------------
from sklearn.ensemble import RandomForestClassifier
model1=RandomForestClassifier(n_estimators=1000)
model1.fit(X,Y)
importances=model1.feature_importances_
finaldf=pd.DataFrame({"Features":pd.DataFrame(X).columns,"Importances":importances})
finaldf.set_index("Importances")
finaldf=finaldf.sort_values("Importances")
finaldf.plot.bar(color='teal')
plt.xticks(rotation=40)

Chi-square method--------------------------------

from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import chi2
best_features=SelectKBest(score_func=chi2,k=7)
fit=best_features.fit(X,Y)
dfscores=pd.DataFrame(fit.scores_)
dfcolumns=pd.DataFrame(X.columns)
featureScores=pd.concat([dfcolumns,dfscores],axis=1)
featureScores.columns=['factors','score']
featureScores.sort_values('score').sort_values('score')
          
 Principal component analysis============================

from sklearn.decomposition import PCA
import numpy as np
import matplotlib.pyplot as plt
pca_test=PCA(n_components=43)
pca_test.fit(scaled_data)
plt.plot(np.cumsum(pca_test.explained_variance_ratio_))
plt.xlabel('number of components')
plt.ylabel('cum variance')

n_pca_components=15
pca1=PCA(n_components=n_pca_components)
principal=pca1.fit_transform(scaled_data)
principaldf=pd.DataFrame(data=principal,columns=['PC1','PC2','PC3','PC4','PC5','PC6','PC7','PC8','PC9','PC10','P11','P12','PC13','PC14','PC15'])

Machine larning algorithms-------------------------

1)Random Forest

from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
classifier3=RandomForestClassifier()
classifier3.fit(X_train,y_train)
randomprediction=classifier3.predict(X_test)
from sklearn.metrics import accuracy_score
print("accuracy score of svm",accuracy_score(y_test,randomprediction)*100)

(2)
Support Vector machine

from sklearn.svm import SVC
support=SVC()
support.fit(X_train,y_train)
sv_prediction=support.predict(X_test)
print("accuracy score of svm",accuracy_score(y_test,sv_prediction)*100)

(3)
K-Nearest neighbour

from sklearn.neighbors import KNeighborsClassifier
classifier1=KNeighborsClassifier(n_neighbors=23)
classifier1.fit(X_train,y_train.values.ravel())
knn_predictions=classifier1.predict(X_test)
print("accuracy_score of knn=",accuracy_score(y_test,knn_predictions)*100)

(4)
Decision Tree
from sklearn.tree import DecisionTreeClassifier
classifier2=DecisionTreeClassifier()
classifier2.fit(X_train,y_train)
decision_prediction=classifier2.predict(X_test)
print("accuracy score of decision tree=",accuracy_score(y_test,decision_prediction)*100)

(5)
Logistic Regression

from sklearn.linear_model import LogisticRegression
model13=LogisticRegression()
model13.fit(X_train,y_train)
X_train_prediction=model13.predict(X_test)
accuracy_score(X_train_prediction,y_test)

(6)
Deep Learning
import tensorflow as tf
tf.random.set_seed(3)
from tensorflow import keras

mod=keras.Sequential([
    keras.layers.Flatten(input_shape=(15,)),
    keras.layers.Dense(10,activation='relu'),
    keras.layers.Dense(2,activation='sigmoid')
])

mod.compile(optimizer='adam',
           loss='sparse_categorical_crossentropy',
           metrics=['accuracy'])

import tensorflow as tf
Early_stopping=tf.keras.callbacks.EarlyStopping(
    monitor="val_loss",
    min_delta=0.0001,
    patience=20,
    verbose=1,
    mode="auto",
    baseline=None,
    restore_best_weights=False,
)

history12=mod.fit(X_train,y_train,validation_split=0.33,epochs=1000,batch_size=10,callbacks=Early_stopping)
loss,accuracy=mod.evaluate(X_test,y_test)

WITHOUT USING DIMENSIONALITY TECHNIQUES

1)Random Forest

from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
classifier3=RandomForestClassifier()
classifier3.fit(X_train,y_train)
randomprediction=classifier3.predict(X_test)
from sklearn.metrics import accuracy_score
print("accuracy score of svm",accuracy_score(y_test,randomprediction)*100)

(2)
Support Vector machine

from sklearn.svm import SVC
support=SVC()
support.fit(X_train,y_train)
sv_prediction=support.predict(X_test)
print("accuracy score of svm",accuracy_score(y_test,sv_prediction)*100)

(3)
K-Nearest neighbour

from sklearn.neighbors import KNeighborsClassifier
classifier1=KNeighborsClassifier(n_neighbors=23)
classifier1.fit(X_train,y_train.values.ravel())
knn_predictions=classifier1.predict(X_test)
print("accuracy_score of knn=",accuracy_score(y_test,knn_predictions)*100)

(4)
Decision Tree
from sklearn.tree import DecisionTreeClassifier
classifier2=DecisionTreeClassifier()
classifier2.fit(X_train,y_train)
decision_prediction=classifier2.predict(X_test)
print("accuracy score of decision tree=",accuracy_score(y_test,decision_prediction)*100)

(5)
Logistic Regression

from sklearn.linear_model import LogisticRegression
model13=LogisticRegression()
model13.fit(X_train,y_train)
X_train_prediction=model13.predict(X_test)
accuracy_score(X_train_prediction,y_test)

(6)
Deep Learning
import tensorflow as tf
tf.random.set_seed(3)
from tensorflow import keras

mod=keras.Sequential([
    keras.layers.Flatten(input_shape=(15,)),
    keras.layers.Dense(10,activation='relu'),
    keras.layers.Dense(2,activation='sigmoid')
])

mod.compile(optimizer='adam',
           loss='sparse_categorical_crossentropy',
           metrics=['accuracy'])

import tensorflow as tf
Early_stopping=tf.keras.callbacks.EarlyStopping(
    monitor="val_loss",
    min_delta=0.0001,
    patience=20,
    verbose=1,
    mode="auto",
    baseline=None,
    restore_best_weights=False,
)

history12=mod.fit(X_train,y_train,validation_split=0.33,epochs=1000,batch_size=10,callbacks=Early_stopping)
loss,accuracy=mod.evaluate(X_test,y_test)

